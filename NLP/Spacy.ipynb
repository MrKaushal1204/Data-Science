{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684100eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90fb1d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc57fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr. Strange loves pav bhaji of mumbai.\n",
      "\n",
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "of\n",
      "mumbai\n",
      ".\n",
      "Hulk loves chat of delhi\n",
      "\n",
      "Hulk\n",
      "loves\n",
      "chat\n",
      "of\n",
      "delhi\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji of mumbai. Hulk loves chat of delhi\")\n",
    "for sentence in doc.sents:\n",
    "    print(sentence)            # Sentence tokenization\n",
    "    print()\n",
    "    for word in sentence:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3089b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d95fd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.',\n",
       " 'Strange loves pav bhaji etc.',\n",
       " 'of mumbai.',\n",
       " 'Hulk loves chat of delhi.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import  sent_tokenize\n",
    "sent_tokenize(\"Dr. Strange loves pav bhaji etc. of mumbai. Hulk loves chat of delhi.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ec9aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdadaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2009dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('''Let's go to N.Y.!''')\n",
    "doc1 = nlp(\"Tony gave two $ to Peter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94a88600",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = doc1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed6ccd7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tony"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44b0da69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "print(type(kk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64e1d81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony\n",
      "gave\n",
      "two\n",
      "$\n",
      "to\n",
      "Peter\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for j in doc1:\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7649122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f376c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " '_bulk_merge',\n",
       " '_context',\n",
       " '_get_array_attrs',\n",
       " '_realloc',\n",
       " '_vector',\n",
       " '_vector_norm',\n",
       " 'cats',\n",
       " 'char_span',\n",
       " 'copy',\n",
       " 'count_by',\n",
       " 'doc',\n",
       " 'ents',\n",
       " 'extend_tensor',\n",
       " 'from_array',\n",
       " 'from_bytes',\n",
       " 'from_dict',\n",
       " 'from_disk',\n",
       " 'from_docs',\n",
       " 'from_json',\n",
       " 'get_extension',\n",
       " 'get_lca_matrix',\n",
       " 'has_annotation',\n",
       " 'has_extension',\n",
       " 'has_unknown_spaces',\n",
       " 'has_vector',\n",
       " 'is_nered',\n",
       " 'is_parsed',\n",
       " 'is_sentenced',\n",
       " 'is_tagged',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'mem',\n",
       " 'noun_chunks',\n",
       " 'noun_chunks_iterator',\n",
       " 'remove_extension',\n",
       " 'retokenize',\n",
       " 'sentiment',\n",
       " 'sents',\n",
       " 'set_ents',\n",
       " 'set_extension',\n",
       " 'similarity',\n",
       " 'spans',\n",
       " 'tensor',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'to_array',\n",
       " 'to_bytes',\n",
       " 'to_dict',\n",
       " 'to_disk',\n",
       " 'to_json',\n",
       " 'to_utf8_array',\n",
       " 'user_data',\n",
       " 'user_hooks',\n",
       " 'user_span_hooks',\n",
       " 'user_token_hooks',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c2a6b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "go\n",
      "to\n",
      "N.Y.\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for i in doc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722b877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff5c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066e0e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf5a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f23a3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0233e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5771208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr.\n",
      "Strange\n",
      "loves\n",
      "pav\n",
      "bhaji\n",
      "etc\n",
      ".\n",
      "of\n",
      "mumbai\n",
      ".\n",
      "Hulk\n",
      "loves\n",
      "chat\n",
      "of\n",
      "delhi\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"en\")    # Selecting the language.\n",
    "doc = nlp(\"Dr. Strange loves pav bhaji etc. of mumbai. Hulk loves chat of delhi.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e029020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hulk"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1089d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b45bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25af175c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af12823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e22ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd720946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc93e466",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8d44f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b7e1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688ded3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11214b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779784d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140f3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14cd98a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ade36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6d6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29433ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb792c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d927d536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2512a62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translating String to Morse Code\n",
      "\n",
      "Enter a word or a sentence: kaushal.\n",
      "-.- .- ..- ... .... .- .-.. \n",
      "Invalid characters found - refer to morse code alphabet\n"
     ]
    }
   ],
   "source": [
    "def string_to_morse_code():\n",
    "\n",
    "    print(\"Translating String to Morse Code\\n\")\n",
    "\n",
    "    words = input(\"Enter a word or a sentence: \")\n",
    "    words = words.upper()\n",
    "\n",
    "    codeDict = createDict()\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    try:\n",
    "        while (i < len(words)):\n",
    "            if (words[i] != \" \"):\n",
    "                print(codeDict[words[i]], end = \" \")\n",
    "            else:\n",
    "                print(\"/\", end = \" \")\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"\\nInvalid characters found - refer to morse code alphabet\")\n",
    "\n",
    "\n",
    "def createDict():\n",
    "    code = {'A': '.-',\n",
    "            'B': '-...',\n",
    "            'C': '-.-.',\n",
    "            'D': '-..',\n",
    "            'E': '.',\n",
    "            'F': '..-.',\n",
    "            'G': '--.',\n",
    "            'H': '....',\n",
    "            'I': '..',\n",
    "            'J': '.---',\n",
    "            'K': '-.-',\n",
    "            'L': '.-..',\n",
    "            'M': '--',\n",
    "            'N': '-.',\n",
    "            'O': '---',\n",
    "            'P': '.--.',\n",
    "            'Q': '--.-',\n",
    "            'R': '.-.',\n",
    "            'S': '...',\n",
    "            'T': '-',\n",
    "            'U': '..-',\n",
    "            'V': '...-',\n",
    "            'W': '.--',\n",
    "            'X': '-..-',\n",
    "            'Y': '-.--',\n",
    "            'Z': '--..',\n",
    "            '1': '.----',\n",
    "            '2': '..---',\n",
    "            '3': '...--',\n",
    "            '4': '....-',\n",
    "            '5': '.....',\n",
    "            '6': '-....',\n",
    "            '7': '--...',\n",
    "            '8': '---..',\n",
    "            '9': '----.',\n",
    "            '0': '-----'}\n",
    "\n",
    "    return code\n",
    "\n",
    "\n",
    "string_to_morse_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fb3f2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def findMajority(arr, n):\n",
    " \n",
    "    maxCount = 0\n",
    "    index = -1  # sentinels\n",
    "    for i in range(n):\n",
    " \n",
    "        count = 0\n",
    "        for j in range(n):\n",
    " \n",
    "            if(arr[i] == arr[j]):\n",
    "                count += 1\n",
    " \n",
    "        # update maxCount if count of\n",
    "        # current element is greater\n",
    "        if(count > maxCount):\n",
    " \n",
    "            maxCount = count\n",
    "            index = i\n",
    " \n",
    "    # if maxCount is greater than n/2\n",
    "    # return the corresponding element\n",
    "    if (maxCount > n//2):\n",
    "        print(arr[index])\n",
    " \n",
    "    else:\n",
    "        print(\"No Majority Element\")\n",
    " \n",
    " \n",
    "\n",
    "arr = [1, 1, 2, 1, 3, 5, 1]\n",
    "n = len(arr)\n",
    "\n",
    "# Function calling\n",
    "findMajority(arr, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4bf69cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def get_majority(arr):\n",
    "    dict = {}\n",
    "    for i in arr:\n",
    "        dict[i] = arr.count(i)\n",
    "    \n",
    "    majotrity = max(dict.values())\n",
    "    for j in dict:\n",
    "        if dict[j] == majotrity:\n",
    "            print(j)\n",
    "    \n",
    "\n",
    "arr = [1, 1, 2, 1, 3, 5, 1]\n",
    "\n",
    "get_majority(arr)\n",
    "# token = 7338127071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2c91a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter word or sentencekaushal kihsot\n",
      "kaushal kihsot\n"
     ]
    }
   ],
   "source": [
    "def censor(text, word):\n",
    " \n",
    "    # Break down sentence by ' ' spaces\n",
    "    # and store each individual word in\n",
    "    # a different list\n",
    "    word_list = text.split()\n",
    " \n",
    "    # A new string to store the result\n",
    "    result = ''\n",
    " \n",
    "    # Creating the censor which is an asterisks\n",
    "    # \"*\" text of the length of censor word\n",
    "    stars = '*' * len(word)\n",
    " \n",
    "    # count variable to\n",
    "    # access our word_list\n",
    "    count = 0\n",
    " \n",
    "    # Iterating through our list\n",
    "    # of extracted words\n",
    "    index = 0;\n",
    "    for i in word_list:\n",
    " \n",
    "        if i == word:\n",
    "             \n",
    "            # changing the censored word to\n",
    "            # created asterisks censor\n",
    "            word_list[index] = stars\n",
    "        index += 1\n",
    " \n",
    "    # join the words\n",
    "    result =' '.join(word_list)\n",
    " \n",
    "    return result\n",
    " \n",
    "     \n",
    "extract = input(\"Enter word or sentence\")             \n",
    "cen = \"computer\"   \n",
    "print(censor(extract, cen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58dcd41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "token = [token.text for token in doc]\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "834ce24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gim', 'me', 'double', 'cheese', 'extra', 'large', 'healthy', 'pizza']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "nlp.tokenizer.add_special_case(\"gimme\",[\n",
    "    {ORTH:\"gim\"},\n",
    "    {ORTH:\"me\"}\n",
    "])\n",
    "\n",
    "doc = nlp(\"gimme double cheese extra large healthy pizza\")\n",
    "\n",
    "token = [token.text for token in doc]\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "10cfa937",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "Look for data to help you address the question. Governments are good\n",
    "sources because data from public research is often freely available. Good\n",
    "places to start include http://www.data.gov/, and http://www.science.\n",
    "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
    "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/, \n",
    "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
    "'''\n",
    "# text = text.strip()\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d321709b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.data.gov/',\n",
       " 'http://www.science',\n",
       " 'http://data.gov.uk/.',\n",
       " 'http://www3.norc.org/gss+website/',\n",
       " 'http://www.europeansocialsurvey.org/.']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "websites = [token.text for token in doc if token.like_url]\n",
    "websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "855b6f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e096cc5",
   "metadata": {},
   "source": [
    "## Figure out all transactions from this text with amount and currency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3b094ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d698b959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two $\n",
      "500 €\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(transactions)\n",
    "tokens = [token for token in doc]\n",
    "for t in tokens:\n",
    "    if t.is_currency:\n",
    "        print(tokens[t.i-1],t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b9f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
